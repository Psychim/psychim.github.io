---
layout: post
title:  "神经网络笔记"
categories: ml
tags: ml
---
# 激活函数
## Sigmoid
$$\sigma(x)=\frac{1}{1+e^{-x}}$$  
有两个缺点：
1. 输入过大时，梯度会消失
2. 不是以0为中心的。输出总是大于0，导致下一个神经元内的参数梯度要么同正要么同负。但是在平均了多个样本的梯度后可以缓解该问题。

## tanh
$$\tanh{(x)}=2\sigma(2x)-1$$  
解决了sigmoid函数的缺点2，但仍然有缺点1。
# ReLU
$$ ReLU(x)=max(0,x)$$  
输入很大时不会有梯度消失问题，但是x<0时，梯度变为0，可能导致神经元死亡（即不再更新）。  
收敛的效率要比tanh快6倍。
# Leaky ReLU
$$ Leaky\ ReLU(x)=I(x<0)(\alpha x)+I(x\geq0)(x)$$  
可以解决ReLU的问题，但不能保证总能解决。
## Maxout
$$ Maxout(x)=max(w_1x+b_1,w_2x+b_2)$$  
可以解决上述所有激活函数的问题，但参数量增加了一倍。
## 如何选取激活函数？
Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of “dead” units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout.
# 神经网络的表达能力
含有一层隐藏层的神经网络即可近似任意连续函数(Neural network is a universal approximator)，用数学语言描述为：  
给定任意连续函数$f(x)$和某个$\epsilon > 0$，存在单层神经网络$g(x)$,使得$\forall x ,|f(x)-g(x)|<\epsilon$  
越深层的神经网络则越能用好的平滑函数拟合数据的统计性质。这属于一种实践经验，实际上它们的表达能力是等价的。个人认为，在实践中要做的是拟合有限数据背后隐含的函数，而非给定一个函数去拟合这个函数本身。因此泛化能力才决定一个神经网络的好坏。虽然深浅层神经网络的表达能力相同，但深层神经网络的泛化能力更强。而且浅层的神经网络在学习时更容易陷入较差的局部最优，无法达到它表达能力的上限。
# gradient check
在数值上验证梯度是否计算正确：  
$$f'(\theta)=\frac{J(\theta+\epsilon)-J(\theta-\epsilon)}{2\epsilon}$$  
# 数据预处理
## Mean Subtraction
所有输入数据（包括训练、验证、测试）减去训练数据的平均值，使得数据以零点为中心
## Normalization
所有输入数据除以训练数据的标准差。
## Whitening
1. 用均值减去$X$得到$X'$
2. 对$X'$做SVD得到$U,S,V$
3. 计算$UX'$，每一维除以$S$中对应的奇异值(为0则除以一个极小值)  

作用是让输入的协方差矩阵变成单位矩阵。
# 参数初始化
对sigmoid和tanh激活函数，采用以下均匀分布初始化参数：  
$$W\sim U\left[-\sqrt{\frac{6}{n^{(l)}+n^{(l+1)}}},\sqrt{\frac{6}{n^{(l)}+n^{(l+1)}}}\right], bias=0$$  
该初始化可令参数方差在backprop过程中保持不变
# 学习率
## 学习率衰减策略
`指数衰减`: $\alpha(t)=\alpha_0 e^{-kt}$，k为超参数  
`xx衰减`: $\alpha(t)=\frac{\alpha_0\tau}{\max (t,\tau)}$，即迭代t超过$\tau$后开始衰减。
## Momentum 
按如下方式更新x：
```
v=mu*v-alpha*grad_x
x+=v
```
## Adaptive Optimization Methods
动态调整每个参数的学习率，更新慢的参数会有更大的学习率
### AdaGrad
$$ cache(t,i)=\sum^t_{\tau=1}g^2_{\tau,i}$$  
$$ \theta_{t,i}=\theta_{t-1,i}-\frac{\alpha}{\sqrt{cache(t,i)}}g_{t,i}$$  
###  RMSProp
$$ cache(t,i)=\beta*cache(t-1)+(1-\beta)*g^2_{t,i}$$  
（即分母改成指数滑动平均）  
$$\theta_{t,i}=\theta_{t-1,i}-\frac{\alpha}{\sqrt{cache(t,i)}+\epsilon} g_{t,i}$$
### Adam
$$m=\beta_1*m+(1-beta_1)*g_{t,i}$$  
$$v=\beta_2*v+(1-beta_2)*g^2_{t,i}$$  
（分母和梯度都改成指数滑动平均）  
$$\theta_{t,i}=\theta_{t-1,i}-\alpha*\frac{m}{\sqrt{v}+\epsilon}$$