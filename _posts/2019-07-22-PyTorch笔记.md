---
layout: post
title:  "PyTorch笔记"
categories: notes
tags: pytorch notes python
---
# 与tensorflow的对比
tensorflow中，需要先定义静态的计算图，每次执行时，只有输入的数据不同，而计算图相同；
而pytorch中，每次执行都需要重新定义动态的计算图。  
静态图的好处是可以事先最优化图的结构或定义好计算结点的分布。坏处是面对一些结构与数据相关的模型比较麻烦。比如RNN的长度可能随样本长度而动态地改变，这需要通过for循环实现，在tensorflow中，for循环需要嵌入到图中，成为图的一个子结构；但在pytorch中，只需要用普通的for循环来处理。

# 自定义autograd.Function
相当于自定义一种新的计算图节点类型，在backprop时可以自动完成梯度的传递。
## 定义
```
class MyAutoGrad(torch.autograd.Function):
    @staticmethod
    def forward(ctx,input):
        """
            根据输入input计算输出output,ctx是一个context对象，可用于存储backward需要的信息
        """
        ctx.save_for_backward(input)
        return output
    @staticmethod
    def backward(ctx,grad_output):
        """
            该函数根据输出的梯度grad_output计算输入的梯度grad_input
        """
        input,=ctx.saved_tensors
        """
            计算input的梯度grad_input
        """
        return grad_input
```
## 调用
```
func=MyAutoGrad.apply
"""
    输入input
"""
pred=func(input)
"""
    计算loss
"""
loss.backward()   # 自动计算梯度
input
```
## 如何避免梯度的追踪
用梯度更新参数时必须关闭梯度的追踪
```
a.
with torch.no_grad():
    更新tensor
b.
通过tensor.data更新
```
# nn package
包含了常用的神经网络结构，损失函数等
## nn.Module
相当于将若干个节点组合成一个可复用的模块，同时可能包含可学习的参数，可通过继承该类实现自定义。
### 实例
（括号里是常用的参数，不代表所有参数）
```
nn.Sequential(m1,m2,...) 顺次连接若干个层
nn.Linear(d_in,d_out) 全连接层
nn.ReLU() ReLu激活函数
```
### 成员
```
Module.parameters() 包含了所有参数，可迭代
```
## 损失函数
```
nn.MSELoss(reduction='sum') 最小平方误差损失（注意是个类）
```
# optim package
包含了常见的最优化算法，封装了参数更新等过程
# torch.matmul
对于高维的张量，执行batched matrix multiplication